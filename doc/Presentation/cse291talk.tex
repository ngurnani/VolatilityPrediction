% CSE 291 Fall 2016 - Presentation
% Nishant D. Gurnani

\documentclass{beamer}
\mode<presentation> {
\usetheme{default}
\setbeamertemplate{footline}[page number]
\setbeamercovered{invisible}
% To remove the navigation symbols from the bottom of slides%
\setbeamertemplate{navigation symbols}{} 
}

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}


\input{Commands}
% For citations
\usepackage{natbib}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm} 


\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}

\resetcounteronoverlays{saveenumi}

%\setbeamerfont{institute}{size=\medium}
\setbeamerfont{date}{size=\tiny}


\title[CSE291 Talk]{}
%
\author{}
\institute[]
{
CSE 291 - Advanced Optimization: Theory and Algorithms\\
Nishant D. Gurnani \\
}
\date{November 4, 2016}

\begin{document}

\begin{frame}

\begin{figure}
\centering
\vspace{15pt}
\includegraphics[width=85mm]{title.png}
\end{figure}
\vspace{-65pt}
\titlepage

\end{frame}


% SLIDE 1
\section{Introduction}


% SLIDE 2
\begin{frame}
\frametitle{Non-convex optimization problems are everywhere!}
\vspace{0.1in}
\begin{columns}[T]
\begin{column}{.5\textwidth}
\begin{block}{Examples include:}

\begin{itemize}
\item{Training multi-layer neural networks $\leftarrow$ (focus of this talk)}
\item{Maximum likelihood estimation of latent variable models}
\item{Clustering: $k$-means, hierarchical}
\end{itemize}
\end{block}
\end{column}

\begin{column}{.5\textwidth}
\begin{block}{}
\includegraphics[width=40mm]{neuralnetwork}
\end{block}
\vspace{-0.25in}
\begin{block}{}
\includegraphics[width=33mm]{graphmodel.png}
\end{block}
\vspace{-0.2in}
\begin{block}{}
\includegraphics[width=35mm]{cluster.png}
\end{block}
\end{column}
\end{columns}
\end{frame}


% SLIDE 3
\begin{frame}
\frametitle{Convex vs. Non-convex Optimization}

\vspace{-0.18in}

\begin{columns}[T]
\begin{column}{.5\textwidth}
\begin{block}{}

\center
\includegraphics[width=60mm]{convexpic}

\begin{itemize}
\item{Unique global optimum}
\end{itemize}
\end{block}
\end{column}

\begin{column}{.5\textwidth}
\begin{block}{}
\center
\includegraphics[width=60mm]{nonconvexpic}

\begin{itemize}
\item{Multiple local optima, saddle points}
\end{itemize}
\end{block}
\end{column}
\end{columns}

\end{frame}


% SLIDE 4
\begin{frame}
\frametitle{Non-convex more prevalent!}
\center
\includegraphics[width=100mm]{nonconvex_iceburg}
\end{frame}

% SLIDE 5
\begin{frame}
\frametitle{What makes training Neural Networks hard?}
\vspace{5pt}

\pause
\begin{block}{Potential Problems:}
\begin{itemize}
\pause
\item{no convergence guarantees}
\pause
\item{proliferation of local minima}
\pause
\item{saddle points}
\pause
\end{itemize}
\end{block}

\begin{block}{Our contribution:}
\begin{itemize}
\pause
\item{geometric intuition derived from low dimensional spaces, doesn't generalize to high dimensional spaces}
\pause
\item{consequently, we argue that the \textcolor{red}{\textbf{proliferation of saddle points}}, not local minima is the main source of difficulty}
\end{itemize}
\end{block}

\end{frame}

% SLIDE 6
\begin{frame}
\frametitle{Related work}
Prior to this work, there was very little related literature.
\vspace{0.25in}
\begin{itemize}
\item{\textit{Neural networks and principal component analysis: Learning from examples without local minima}, Baldi and Hornik (1989)}
\vspace{0.05in}
\item{\textit{On the saddle point problem for non-convex optimization}, Pascanu, Dauphin, Ganguli and Bengio (arXiv May 2014)}
\end{itemize}
\end{frame}

% SLIDE 7
\begin{frame}
\frametitle{Outline}
\tableofcontents[pausesections]
\end{frame}

% SLIDE 8
\section{Different Types of Critical Points}
\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}


% SLIDE 9
\begin{frame}
\frametitle{Different Types of Critical Points}
\vspace{0.15in}
1st order \textcolor{red}{critical points} ($\nabla f(\theta) = 0$) of $f(\theta)$ can be characterized by the curvature of the function in its vicinity, as described by the \textcolor{red}{eigenvalues of $\bm{H}$}, the Hessian matrix.

\vspace{4pt}
\begin{itemize}
\pause
\item{ if $\forall_i \lambda_i > 0 \rightarrow $ local minimum}
\pause
\item{ if $\forall_i \lambda_i < 0 \rightarrow $ local maximum}
\pause
\item{ if $\exists i$ s.t. $\lambda_i > 0$, $\lambda_i <0$ and $\lambda_i \neq 0$  $\rightarrow $ saddle point with min-max structure}
\pause
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.88\textwidth]{different_critical_points.png}
\end{figure}

\end{frame}

% SLIDE 10
\begin{frame}
\frametitle{Different Types of Saddle Points}
If $\bm{H}$ is singular ($\exists \lambda_i = 0$), then the $degenerate$ 1st order critical point can also be a saddle point.
\pause
\vspace{-13pt}
\begin{figure}
  \centering
    \subfloat[Min-max saddle ($\lambda_i > 0$, $\lambda_i <0$ and $\lambda_i \neq 0$)]{\includegraphics[height=4cm,width=5cm]{minmaxsaddle.png}}
    \hspace{4pt}
  \subfloat[Monkey saddle (min-max structure and 0 eigenvalue)]{\includegraphics[height=4cm,width=5cm]{monkeysaddle.png}}\qquad
\end{figure}
\pause
For purposes of this work, we focus on non-degenerate 1st order saddle points for which the Hessian is not singular.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prevalence of Saddle Points in High Dimensions}
% SLIDE 11
\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}


% SLIDE 12 - Ratio of Number of Saddle Points to Local Minima
\begin{frame}
\frametitle{Saddle points are prevalent in high dimensions}
\pause
\begin{block}{Claim}
The ratio of the number of saddle points to local minima increases exponentially with dimensionality N.
\end{block}
\pause
\begin{block}{Intuition}
\vspace{-8pt}
\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{simulation.png}
\end{figure}
\end{block}

\end{frame}

% SLIDE 13
\begin{frame}
\frametitle{Terminology}

We plan to use some results from statistical physics, but first we define two key terms.
\pause
\vspace{0.05in}
\begin{block}{Index of a critical point ($\alpha$)}
$\#$ of negative eigenvalues of $\bm{H}$ at a given critical point
\end{block}
\pause
\vspace{0.05in}
\begin{block}{Gaussian Random Field}
set of normally distributed random variables $Y(\bm{x}), \bm{x} \in \mathbb{R}^n$, with a collection of distribution functions $ F(Y(\bm{x_1}) \leq y_1, \dots, Y(\bm{x_n}) \leq y_n) $

where the $\bm{x_i}$ can be points on some manifold
\vspace{-0.1in}
\center
\includegraphics[width=0.50\textwidth]{gaussrandomfield.png}

\end{block}

\end{frame}


\begin{frame}
\frametitle{Related Studies}

In \textit{Bray \& Dean 2007}, the authors calculate the average number of critical points of a Gaussian field on high-dimensional space as a function of their energy and index.

\vspace{0.1in}
\pause
We use two specific theoretical results from their work:

\begin{enumerate}
\pause
\item{eigenvalues of $\bm{H}$ at a critical point are distributed according to Wigner's famous semicircular law but shifted by an amount determined by $\epsilon$}
\seti
\end{enumerate}

\center
\includegraphics[height=4cm,width=4.5cm]{wigner}
\end{frame}

\begin{frame}
\frametitle{Theoretical Prediction for GRF}

\begin{enumerate}
\conti
\item{In the $\epsilon$ (training error) vs. $\alpha$ (index of critical point) plane, the critical points concentrate on a monotonically increasing curve as $\alpha$ ranges from 0 to 1}
\end{enumerate}
\vspace{-0.1in}
\center
\includegraphics[height=6.5cm,width=8.5cm]{monotone.png}

\end{frame}

\begin{frame}
\frametitle{Experimental Validation}
\pause
We want to experimentally test whether the theoretical predictions for Gaussian random fields presented by Bray and Dean (2007) hold for neural networks.
\pause
\vspace{0.2in}
\begin{block}{Goals}
\begin{enumerate}
\pause
\item{explore how critical points of a single layer MLP are distributed in the $\epsilon$-$\alpha$ plane}
\vspace{0.05in}
\pause
\item{explore how the eigenvalues of $\bm{H}$ at these critical points are distributed}
\end{enumerate}
\end{block}

\end{frame}

\begin{frame}
\frametitle{Experimental Methodology}
\pause
We use a small MLP trained on a down-sampled version of MNIST and CIFAR-10
\pause
\begin{block}{MNIST}
\begin{itemize}
\item{Use a single layer MLP and first use our ideal algorithm (SFN) to define an ideal training path (store all parameters, repeat many times)}
\item{Using the stored parameters, repeat the process adding some noise and use the Newton method to discover nearby critical points along the ideal training path}
\end{itemize}
\end{block}

\vspace{-0.1in}
\pause
\begin{block}{CIFAR-10}
\begin{itemize}
\item{Train multiple 3-layer deep neural networks using SGD and save the parameters for each epoch}
\item{We then train using the Newton method to discover nearby critical points along the ideal training path}
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\frametitle{Saddle Point Experiments}
\vspace{0.05in}
\begin{figure}
\center
\includegraphics[width=1.1\textwidth]{saddlepoint_experiment1_mnist.png}
\caption{single layer MLP trained using Newton method on a down-sampled version of MNIST (y axes are in logarithmic scale)}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Saddle Point Experiments}
\begin{figure}
\center
\includegraphics[width=1.1\textwidth]{saddlepoint_experiment1_cifar10.png}
\caption{three layer MLP trained using SGD on a down-sampled version of CIFAR-10 (y axes are in logarithmic scale)}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dynamics of Optimization Algorithms near Saddle Points}
\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\frametitle{Dynamics of Optimization Algorithms near Saddle Points}
\pause
\begin{itemize}
\item{Given the prevalence of saddle points, we want to understand how various optimization algorithms behave near them}
\pause
\item{We focus on non-degenerate saddle points for which $\bm{H}$ is not singular}
\end{itemize}
\pause
\vspace{-0.2in}
\begin{block}{}
To locally analyze these critical points we reparametrize our function $f$:
\vspace{-0.05in}
$$ f(\theta_0 + \Delta\theta) = f(\theta_0) + \frac{1}{2}\sum_{i=1}^{n_{\theta}}\textcolor{red}{\lambda_i \Delta \bm{v_{i}^2} }$$
\end{block}


$\lambda_i$  - $i$th eigenvalue of $\bm{H}$ \\
\vspace{0.04in}
$\Delta \bm{v_{i}} = (\bm{e_i}^{T} \Delta \theta)$\\

%$\Delta \bm{v} = \frac{1}{2} \begin{bmatrix} \bm{e_1} & \hdots & \bm{e_{n_{\theta}}} \end{bmatrix}^T \Delta\theta$\\
%\vspace{0.04in}
%$\Delta \bm{v_{i}}$ - new parameters of the model corresponding to motion along the eigenvectors $\bm{e_i}$ of the Hessian of $f$ at $\theta_0$ \\

\end{frame}

\begin{frame}
\frametitle{Gradient Descent near saddle points}
\pause
\begin{itemize}
\item{A step of gradient descent always points away from the saddle close to it}
\pause
\item{Drawback near saddles is not the direction but the step size $-\lambda_i \Delta \bm{v_i}$ as small steps are taken in directions corresponding to small eigenvalues}
\pause
\end{itemize}

\vspace{-0.2in}
\begin{figure}
  \centering
  \subfloat[SGD (red dots) slows down near the saddle point]{\includegraphics[height=3cm,width=5cm]{gradient_descent1.png}}
  \hspace{0.02in}
  \subfloat[SGD (green dots) particularly good at exploiting the unstable nature of a saddle point]{\includegraphics[height=3cm,width=5cm]{gradient_descent2.png}}
\end{figure}

\end{frame}

\begin{frame}
\frametitle{Newton's method near saddle points}
\pause
\begin{itemize}
\item{Newton's method solves the slowness problem by rescaling gradients in each direction with inverse of the corresponding eigenvalue (yielding step $-\Delta \bm{v_i}$)}
\pause
\vspace{0.04in}
\item{if however, $\lambda_i < 0$ then $$ GD: - (-\lambda_i)\Delta \bm{v_i} = \lambda_i \Delta \bm{v_i}$$ $$Newton: - (-\lambda_i)\frac{1}{-\lambda_{i}}\Delta\bm{v_i} = -\Delta\bm{v_i} $$}
\pause   
\end{itemize}

\vspace{-0.44in}
\begin{figure}
\centering
\includegraphics[height=4cm, width=11cm]{newtonmethod.png}
\vspace{-0.15in}
\caption{In (b) $\theta_0$ becomes an attractor for the Newton method, which can get stuck in this saddle point and not converge to a local minima}
\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Attacking the Saddle Point Problem}
\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}


\begin{frame}
\frametitle{How would an optimal algorithm behave?}
\vspace{-0.25in}
\pause
\begin{itemize}
\item{Analysis of Newton's method suggests a simple heuristic solution:

$$ \text{Newton method: \hspace{0.04in}} x_{t+1} = x_t - \bm{H}^{-1} \Delta f(x_t) $$
\pause
$$ \text{Proposed solution: \hspace{0.04in}} x_{t+1} = x_t - |\bm{H}|^{-1} \Delta f(x_t)$$
\vspace{0.06in}
where $|\bm{H}| = \sum_i |\lambda_i| \bm{e_i}$ 
}
\pause
\vspace{0.1in}
\item{We show that this heuristic solution arises naturally from a generalized trust region approach}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generalized Trust Region Approach}

\vspace{-0.25in}

In order to attack the saddle point problem, we define a class of generalized trust region methods (think constrained optimization problem)
\pause
\vspace{0.25in}
\begin{itemize}

\item{Letting $\tau_{k}(f,\theta,\Delta \theta)$ be the k-order Taylor series expansion of $f$ around $\theta$ evaluated at $\theta + \Delta\theta$, we get:

$$ \Delta \theta = \argmin_{\Delta \theta} \tau_{k}(f,\theta,\Delta \theta) $$ $$\text{ with } k \in \{1,2\} \text{ s.t. } d(\theta,\theta+\Delta\theta) \leq \Delta$$}

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Generalized Trust Region Approach}

\vspace{-0.1in}

\begin{itemize}
\item{We consider minimizing the 1st order Taylor expansion, and pick a suitable distance measure $d$ that aims to give us some curvature information for $f$}
\pause
\vspace{0.1in}

\item{We do this by bounding the discrepancy between the first and second order taylor expansions of $f$: $$ d(\theta, \theta + \Delta \theta) = \Bigl| f(\theta) + \nabla f \Delta\theta + \frac{1}{2}\Delta\theta^{T}\bm{H}\Delta\theta - f(\theta) - \nabla f \Delta\theta \Bigr|$$
$$ = \frac{1}{2} \Bigl | \Delta\theta^T \bm{H} \Delta\theta \Bigr | \leq \Delta $$}

\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Generalized Trust Region Approach}

\begin{itemize}
\item{We bound the distance measure further by $\Delta\theta^{T} |\bm{H}| \Delta\theta$ which results in the following generalized trust region method:

$$ \Delta\theta = \argmin_{\Delta\theta} f(\theta) + \nabla f \Delta\theta \text{ s.t. } \Delta\theta^{T} |\bm{H}| \Delta\theta \leq \Delta $$}
\pause
\item{Solving the constrained optimization using Lagrange multiplier yields a step of the form: $$\Delta\theta = - \nabla f |\bm{H}|^{-1}$$ which is exactly our proposed heuristic!}
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Saddle-free Newton (SFN) Method}
\vspace{0.07in}
Subsequently, we propose the saddle-free Newton method which is identical to the Newton method when $\bm{H}$ is positive definite, but unlike the Newton method, it can escape saddle points.
\begin{figure}
\includegraphics[width=80mm]{saddlefree.png}
\end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}
\frametitle{Existence of Saddle Points in Neural Networks}
\pause
\begin{block}{Goals}

\begin{itemize}
\pause
\item{validate existence of saddle points in neural networks}
\pause
\item{want to observe behavior of algorithms we described earlier}
\end{itemize}
\pause
\end{block}

\begin{block}{Methodology}
\begin{itemize}
\pause
\item{Train on down-sampled versions of MNIST and CIFAR-10, where we can compute the update directions by each algorithm exactly}
\pause
\item{Compare MSGD, damped Newton and SFN}
\end{itemize}

\end{block}

\end{frame}


\begin{frame}
\frametitle{Experimental validation of the saddle-free Newton method}
\begin{figure}
\center
\includegraphics[width=115mm]{Experiment1.png}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Effectiveness of saddle-free Newton}
\pause
\begin{block}{Goals}
\begin{itemize}
\pause
\item{Want to test effectiveness of saddle-free Newton method on larger neural nets}
\pause
\item{Exact implementation of SFN is intractable in high-dimensional problem, need a proxy to test effectiveness}
\pause
\end{itemize}
\end{block}

\begin{block}{Methodology}
\begin{itemize}
\pause
\item{We test effectiveness of SFN on a deep autoencoder (using all MNIST data) and recurrent neural net (using Penn Treebank corpus)}
\pause
\item{In each case we train the model with SGD and wait until learning stalls, we then continue training with SFN}
\pause
\item{We can't exactly compute $\bm{H}$ in the high-dimensional problem so we optimize in a lower-dimensional Krylov subspace}
\end{itemize}
\end{block}

\end{frame}


\begin{frame}
\frametitle{Experimental validation of the saddle-free Newton method}
\begin{figure}
\includegraphics[width=115mm,height=60mm]{Experiment2.png}
\end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion}
\begin{frame}
\frametitle{Outline}
\tableofcontents[currentsection]
\end{frame}


\begin{frame}
\frametitle{Conclusion}
\pause
\begin{itemize}
\item{Argued about the prevalence of saddle points and proposed a very simple algorithm saddle-free Newton method to deal with them}
\pause
\vspace{0.04in}
\item{Method is highly impractical as exact implementation is intractable in a high dimensional problem} 
\pause
\vspace{0.04in}
\item{Experimental evidence limited and performed only using small networks}
\vspace{0.04in}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Subsequent Work}
\begin{itemize}
\pause
\item{Despite issues, rather important paper that accelerated work in saddle point behavior for non-convex optimization}
\vspace{0.04in}
\pause
\item{Choromanska et. al (AISTATS 2015) showed a connection between deep networks with
ReLU and spherical spin-glass model}
\vspace{0.04in}
\pause
\item{Ge. et al.(COLT 2015) introduced notion of strict saddle property for non-convex problem and showed that stochastic gradient descent converges to a local minimum in a polynomial number of iterations}
\vspace{0.04in}
\end{itemize}
\end{frame}

\end{document}